---
title: "P8105 - Homework 3"
author: "Mingkuan Xu"
output: github_document
---
```{r library}
library(tidyverse)
library(p8105.datasets)
```

## Problem 1

#### Part 1 - Data import & description
```{r p1_import}
data("instacart")
head(instacart)
```

This is an anonymized dataset recording online grocery orders from Instacart users. Each row in the dataset is a product from an order; there is a single order per user in this dataset. There are `r nrow(instacart)` total rows in this dataset with `r ncol(instacart)` columns, recording `r length(unique(pull(instacart,order_id)))` orders in total. The recorded information include: `r tolower(names(janitor::clean_names(instacart,"sentence")))`. There are some interesting observations of this dataset. For example, there are `r length(unique(pull(instacart,department)))` departments in total, including `r unique(pull(instacart,department))`.

#### Part 2 - Analysis of aisles

```{r p1_part2}
instacart %>% pull(aisle) %>% unique() %>% length() # Find number of aisles.

instacart_by_aisle = 
  instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) 

head(instacart_by_aisle,1)# Find most popular aisle
```

Using the codes above, we found that there are 134 aisles in total, with aisle of the fresh vegetables being the most popular one from which 150609 items were ordered.

#### Part 3 - Plot of items from aisles.

Now we plot the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered using the following code.

```{r p1_part3}
instacart_by_aisle %>% 
  filter(n>10000)  %>%
  ggplot(.,aes(x=reorder(aisle,n),y=n)) + 
  geom_bar(stat="identity") +
  xlab("Aisle")+
  ylab("Number of Items") +
  coord_flip()
```

#### Part 4 - Three most popular items

Using the code below, we display the table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r p1_part4}
baking_ingredients = 
  instacart %>%
  filter(aisle=="baking ingredients") %>%
  count(product_name) %>%
  arrange(desc(n)) %>%
  mutate(Aisle="Baking Ingredients") %>%
  rename(Product=product_name,Count=n)%>%
  relocate(Aisle) %>%
  head(3)
  

dog_food_care = 
  instacart %>%
  filter(aisle=="dog food care") %>%
  count(product_name) %>%
  arrange(desc(n)) %>%
  mutate(Aisle="Dog Food Care") %>%
  rename(Product=product_name,Count=n)%>%
  relocate(Aisle) %>%
  head(3)

packaged_vegetables_fruits = 
  instacart %>%
  filter(aisle=="packaged vegetables fruits") %>%
  count(product_name) %>%
  arrange(desc(n)) %>%
  mutate(Aisle="Packaged Vegetables Fruits") %>%
  rename(Product=product_name,Count=n)%>%
  relocate(Aisle) %>%
  head(3)

popular_table = 
  bind_rows(baking_ingredients,
                          dog_food_care,
                          packaged_vegetables_fruits) %>%
  arrange(desc(Count))

popular_table %>% knitr::kable(digits = 1)
```

#### Part 5 - The mean hour of the day

Using the codes below, we display the table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered, from Sunday to Saturday.

```{r p1_part5}

pink_lady_apples = 
  instacart %>%
  filter(product_name == "Pink Lady Apples") %>%
  select(order_dow,order_hour_of_day) %>%
  group_by(order_dow) %>%
  summarize(
    median_hour_of_day = mean(order_hour_of_day)
  ) %>%
  rename("Pink Lady Apples"=median_hour_of_day)


coffee_ice_cream = 
  instacart %>%
  filter(product_name == "Coffee Ice Cream") %>%
  select(order_dow,order_hour_of_day) %>%
  group_by(order_dow) %>%
  summarize(
    median_hour_of_day = mean(order_hour_of_day)
  ) %>%
  rename("Coffee Ice Cream"=median_hour_of_day)

median_hour_table = 
  left_join(pink_lady_apples,coffee_ice_cream,by="order_dow") %>%
  select(-order_dow)
# mutate("Day of Week" = c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))

median_hour_table %>% knitr::kable(digits = 1)
```

## Problem 2

#### Part 1 - Data preprocessing

We start by using the following codes to tide up the dataset.
```{r p2_import}
data("brfss_smart2010")

data_brfss = brfss_smart2010 %>%
  janitor::clean_names() %>% 
  filter(topic=="Overall Health") %>%
  filter(response %in% c("Excellent","Very good","Good","Fair","Poor")) %>%
  mutate(response = factor(response,levels = c("Poor","Fair","Good","Very good","Excellent")))

head(data_brfss) 
```
#### Part 2 - Which states were observed at 7 or more locations?

```{r p2_part2}
data_brfss_2002 = 
  data_brfss %>%
  filter(year==2002) %>%
  select(locationabbr,locationdesc) %>%
  distinct() %>%
  count(locationabbr) %>%
  filter(n>=7)

data_brfss_2010 = 
  data_brfss %>%
  filter(year==2010) %>%
  select(locationabbr,locationdesc) %>%
  distinct() %>%
  count(locationabbr) %>%
  filter(n>=7)

```
We use the above code o find out which states were observed at 7 or more locations in 2002 and 2010. In conclusion, in 2002, the abbreviated names of those states are `r pull(data_brfss_2002,locationabbr)`; in 2010, those states are 
`r pull(data_brfss_2010,locationabbr)`. We observed significantly more locations recorded in 2010 compared with 2002.

#### Part 3 - The “spaghetti” plot 

```{r p2_part3}
data_brfss_excellent = 
  data_brfss %>%
  filter(response=="Excellent") %>%
  drop_na(data_value) %>%
  group_by(year,locationabbr) %>%
  summarize(
    mean_value = mean(data_value)
  ) 
ggplot(data_brfss_excellent,aes(x=year,y=mean_value,color=locationabbr)) + 
  geom_line() +
  geom_point() +
  labs(
    title = "The “Spaghetti” Plot of Excellent Responses (2002~2010)",
    color = "States") +
  ylab("Mean Value")+
  xlab("Year")+
  theme_classic()

  
```

#### Part 4 - The two-panel plot of NY

```{r p2_part4}
data_ny_2006_2010 = 
  data_brfss %>%
  filter(year %in% c(2006,2010),locationabbr=="NY") %>%
  select(year,locationdesc,data_value,response) %>%
  drop_na(data_value)

data_ny_2006_2010 %>%
  ggplot(aes(x=locationdesc,y=data_value)) + 
  geom_bar(aes(fill=response),stat="identity") +
  facet_grid(. ~ year) + 
  coord_flip() +
  theme(legend.position = "bottom") +
  xlab("Data Values")+
  ylab("Locations")+
  labs(title = "A Two-Panel Plot of Responses in NY (2006 vs 2010)")+
  theme_classic()


  
```
## Problem 3

#### Part 1 - Data preprocessing

```{r p3_part1}

# Import data from the csv file. 
data_accel = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(is_weekday=
           ifelse(day %in% c("Saturday","Sunday"),
                  FALSE,
                  TRUE)) %>% # A column to record if it is weekday.
  relocate(week,day_id,day,is_weekday)

head(data_accel)
```

This dataset contains the five weeks of accelerometer data collected on a 63 year-old male with BMI 25.
It records `r nrow(data_accel)` days in total; for each day, it contains the values of `r ncol(select(data_accel,!week:is_weekday))` activities. Aside from the accelerometer data, it also contains `r names(select(data_accel,week:is_weekday))` for each row.

#### Part 2 - Aggregate accross minutes

```{r p3_part2}

# Import data from the csv file. 
data_accel_wholeday = 
  data_accel %>%
  rowwise() %>%
  mutate(activity_total = sum(across(activity_1:activity_1440))) %>%
  ungroup() %>%
  select(day_id,is_weekday,activity_total) 

data_accel_wholeday %>% knitr::kable(digits = 1)

ggplot(data_accel_wholeday,aes(x=day_id,y=activity_total)) + 
  geom_line()+
  geom_point(aes(color=is_weekday))+
  theme_bw()+
  xlab("Days")+
  ylab("Activity Total")+
  labs(color="is_weekday")

  
```
From the data of aggregated total activity, we do not find a trend of overall decreasing or increasing, but we do observe the total values are very unstable from the beginning to the end, with huge variance between days. Additionally, when taking into account the weekday/weekend variable, it looks like after day 20, on each Saturday there will be a huge decrease of the value. 

#### Part 3 - The 24-hour activity time courses for each day
```{r p3_part3}
data_accel_long = data_accel %>% 
  pivot_longer(activity_1:activity_1440,names_to="activity",names_prefix = "activity_") %>%
  mutate(activity = as.numeric(activity))

ggplot(data_accel_long, aes(x=activity,y=value,group=day_id)) + 
  geom_line(aes(color=day))+
  ylab("Average Value")+
  xlab("Minutes")+
  labs(
    title = "The 24-Hour Activity Time Courses for Each Day (in Minute)",
    color = "Days"
  )+
  theme_classic()
```


```{r p3_part3}
data_accel_long_60 = data_accel %>% 
  pivot_longer(activity_1:activity_1440,names_to="activity",names_prefix = "activity_") %>%
  mutate(hour = as.numeric(activity) %/% 60) %>%
  group_by(day_id,day,hour) %>%
  summarize(
    average = mean(value)
  )


ggplot(data_accel_long_60, aes(x=hour,y=average,group=day_id)) + 
  geom_line(aes(color=day))+
  ylab("Average Value")+
  xlab("Hours")+
  labs(
    title = "The 24-Hour Activity Time Courses for Each Day (in Hour)",
    color = "Days"
  )+
  theme_classic()
```

For these two plots, we observe that the values tend to stay in a lower level in the morning, and begin to increase at around 5 am (possibly after getting up) and reach a high level at noon. The values seem to decrease in the afternoon and bounce back in the evening. They drop back to a lower level usually at the end of the day. No obvious patterns related to days were observed.   